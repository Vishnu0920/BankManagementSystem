{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtr9ydcvIyXfburxKi79Wc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishnu0920/BankManagementSystem/blob/main/GNNOnStudentDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 1\n",
        "!pip install pandas torch scikit-learn\n",
        "!pip install pyfm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOGqLsunZD-O",
        "outputId": "57c0634b-a095-432b-9a93-8ce6a02d81ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting pyfm\n",
            "  Downloading pyfm-0.2.4.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyfm) (2.31.0)\n",
            "Collecting urwid>=1.2.1 (from pyfm)\n",
            "  Downloading urwid-2.5.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.6/310.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from urwid>=1.2.1->pyfm) (4.9.0)\n",
            "Building wheels for collected packages: pyfm\n",
            "  Building wheel for pyfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfm: filename=pyfm-0.2.4-py3-none-any.whl size=13207 sha256=c5f8a61ef44af6a6646221a3e15e46ef90e653f8d1c212ec0a5d26c9899cf68d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/5e/81/2218255b6118f1373b7b77b2062d8d2e7f757186cedca7becc\n",
            "Successfully built pyfm\n",
            "Installing collected packages: urwid, pyfm\n",
            "Successfully installed pyfm-0.2.4 urwid-2.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "hDJx8YyMFkY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load the dataset\n",
        "df = pd.read_csv('grade_data.csv')"
      ],
      "metadata": {
        "id": "wQNmOrQYJtWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Encode student_id and course_id using LabelEncoder\n",
        "le_student = LabelEncoder()\n",
        "le_course = LabelEncoder()\n",
        "\n",
        "df['student_id'] = le_student.fit_transform(df['student_id'])\n",
        "df['course_id'] = le_course.fit_transform(df['course_id'])"
      ],
      "metadata": {
        "id": "FZDDVx5OJxOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Map course grades to the range [0, 1] for regression\n",
        "df['course_grade'] = df['course_grade'] / 10.0"
      ],
      "metadata": {
        "id": "a4LcRIl7Kicy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Train-test split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "HgEhjnWNKnZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Convert the data to PyTorch tensors\n",
        "train_user = torch.LongTensor(train_df['student_id'].values)\n",
        "train_course = torch.LongTensor(train_df['course_id'].values)\n",
        "train_grade = torch.FloatTensor(train_df['course_grade'].values)\n",
        "\n",
        "test_user = torch.LongTensor(test_df['student_id'].values)\n",
        "test_course = torch.LongTensor(test_df['course_id'].values)\n",
        "test_grade = torch.FloatTensor(test_df['course_grade'].values)\n"
      ],
      "metadata": {
        "id": "5M6xTCQ6Kq9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NGCF(nn.Module):\n",
        "    def __init__(self, num_users, num_courses, embed_size=64):\n",
        "        super(NGCF, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embed_size)\n",
        "        self.course_embedding = nn.Embedding(num_courses, embed_size)\n",
        "        self.fc1 = nn.Linear(embed_size * 2, embed_size)  # Corrected the input size\n",
        "        self.fc2 = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, user, course):\n",
        "        user_embed = self.user_embedding(user)\n",
        "        course_embed = self.course_embedding(course)\n",
        "\n",
        "        # Concatenate user and course embeddings\n",
        "        combined = torch.cat([user_embed, course_embed], dim=1)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        combined = self.fc1(combined)\n",
        "        combined = nn.ReLU()(combined)\n",
        "        combined = self.fc2(combined)\n",
        "\n",
        "        return combined\n"
      ],
      "metadata": {
        "id": "dsIvnXQuKt87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Hyperparameters\n",
        "num_users = len(le_student.classes_)\n",
        "num_courses = len(le_course.classes_)\n",
        "embedding_dim = 64\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n"
      ],
      "metadata": {
        "id": "g9MXO_IKKwc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Create DataLoader for training\n",
        "train_dataset = TensorDataset(train_user, train_course, train_grade)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "CrR7UKrpK5mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Instantiate the model, define loss function and optimizer\n",
        "model = NGCF(num_users, num_courses, embedding_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Set the print interval\n",
        "print_interval = 10  # Adjust the interval as needed\n"
      ],
      "metadata": {
        "id": "WEqrF0QxK-Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Training loop with performance monitoring\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_index, batch in enumerate(train_dataloader):\n",
        "        user, course, grade = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(user, course).squeeze()\n",
        "\n",
        "        # Ensure output and grade have the same shape\n",
        "        grade = grade.view(-1, 1)  # Reshape grade to (batch_size, 1) to match output shape\n",
        "\n",
        "        loss = criterion(output, grade)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training loss at regular intervals\n",
        "        if batch_index % print_interval == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCfOLu1kLArj",
        "outputId": "c56aea57-159f-4462-ebe0-e674e0cdf50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 0, Loss: 0.6770484447479248\n",
            "Epoch 0, Batch 10, Loss: 0.4091627597808838\n",
            "Epoch 0, Batch 20, Loss: 0.2490861415863037\n",
            "Epoch 0, Batch 30, Loss: 0.15160611271858215\n",
            "Epoch 0, Batch 40, Loss: 0.10299594700336456\n",
            "Epoch 0, Batch 50, Loss: 0.08319690823554993\n",
            "Epoch 0, Batch 60, Loss: 0.06753396987915039\n",
            "Epoch 0, Batch 70, Loss: 0.05856309458613396\n",
            "Epoch 0, Batch 80, Loss: 0.054099924862384796\n",
            "Epoch 0, Batch 90, Loss: 0.0620177760720253\n",
            "Epoch 0, Batch 100, Loss: 0.05191892758011818\n",
            "Epoch 0, Batch 110, Loss: 0.04519454017281532\n",
            "Epoch 0, Batch 120, Loss: 0.04784328490495682\n",
            "Epoch 0, Batch 130, Loss: 0.048547353595495224\n",
            "Epoch 0, Batch 140, Loss: 0.0551079586148262\n",
            "Epoch 0, Batch 150, Loss: 0.03803477808833122\n",
            "Epoch 0, Batch 160, Loss: 0.03584242984652519\n",
            "Epoch 0, Batch 170, Loss: 0.044491082429885864\n",
            "Epoch 0, Batch 180, Loss: 0.04091908782720566\n",
            "Epoch 0, Batch 190, Loss: 0.03533342853188515\n",
            "Epoch 0, Batch 200, Loss: 0.04141280800104141\n",
            "Epoch 0, Batch 210, Loss: 0.04246652126312256\n",
            "Epoch 0, Batch 220, Loss: 0.0336788147687912\n",
            "Epoch 0, Batch 230, Loss: 0.035088006407022476\n",
            "Epoch 0, Batch 240, Loss: 0.026365431025624275\n",
            "Epoch 0, Batch 250, Loss: 0.029378144070506096\n",
            "Epoch 0, Batch 260, Loss: 0.03821606934070587\n",
            "Epoch 0, Batch 270, Loss: 0.028955481946468353\n",
            "Epoch 0, Batch 280, Loss: 0.028740007430315018\n",
            "Epoch 0, Batch 290, Loss: 0.038842007517814636\n",
            "Epoch 0, Batch 300, Loss: 0.04025595635175705\n",
            "Epoch 0, Batch 310, Loss: 0.028043216094374657\n",
            "Epoch 0, Batch 320, Loss: 0.029900509864091873\n",
            "Epoch 0, Batch 330, Loss: 0.021460674703121185\n",
            "Epoch 0, Batch 340, Loss: 0.03181660920381546\n",
            "Epoch 0, Batch 350, Loss: 0.026095420122146606\n",
            "Epoch 0, Batch 360, Loss: 0.0227391067892313\n",
            "Epoch 0, Batch 370, Loss: 0.029553571715950966\n",
            "Epoch 0, Batch 380, Loss: 0.029320720583200455\n",
            "Epoch 0, Batch 390, Loss: 0.025889800861477852\n",
            "Epoch 0, Batch 400, Loss: 0.0173342302441597\n",
            "Epoch 0, Batch 410, Loss: 0.030042611062526703\n",
            "Epoch 0, Batch 420, Loss: 0.02521403506398201\n",
            "Epoch 0, Batch 430, Loss: 0.028463004156947136\n",
            "Epoch 0, Batch 440, Loss: 0.022928522899746895\n",
            "Epoch 0, Batch 450, Loss: 0.019150227308273315\n",
            "Epoch 0, Batch 460, Loss: 0.0243080947548151\n",
            "Epoch 0, Batch 470, Loss: 0.02647976577281952\n",
            "Epoch 0, Batch 480, Loss: 0.021606625989079475\n",
            "Epoch 0, Batch 490, Loss: 0.02511446177959442\n",
            "Epoch 0, Batch 500, Loss: 0.019653333351016045\n",
            "Epoch 0, Batch 510, Loss: 0.02253364585340023\n",
            "Epoch 0, Batch 520, Loss: 0.024086548015475273\n",
            "Epoch 0, Batch 530, Loss: 0.02258930169045925\n",
            "Epoch 0, Batch 540, Loss: 0.025343654677271843\n",
            "Epoch 0, Batch 550, Loss: 0.021171772852540016\n",
            "Epoch 0, Batch 560, Loss: 0.024822546169161797\n",
            "Epoch 0, Batch 570, Loss: 0.023468313738703728\n",
            "Epoch 0, Batch 580, Loss: 0.028270665556192398\n",
            "Epoch 0, Batch 590, Loss: 0.020730679854750633\n",
            "Epoch 0, Batch 600, Loss: 0.026134908199310303\n",
            "Epoch 0, Batch 610, Loss: 0.019412418827414513\n",
            "Epoch 0, Batch 620, Loss: 0.02529042586684227\n",
            "Epoch 0, Batch 630, Loss: 0.021312978118658066\n",
            "Epoch 0, Batch 640, Loss: 0.022374441847205162\n",
            "Epoch 0, Batch 650, Loss: 0.01814001053571701\n",
            "Epoch 0, Batch 660, Loss: 0.020893830806016922\n",
            "Epoch 0, Batch 670, Loss: 0.023139972239732742\n",
            "Epoch 0, Batch 680, Loss: 0.0248397346585989\n",
            "Epoch 0, Batch 690, Loss: 0.02818780019879341\n",
            "Epoch 0, Batch 700, Loss: 0.018409520387649536\n",
            "Epoch 0, Batch 710, Loss: 0.023406051099300385\n",
            "Epoch 1, Batch 0, Loss: 0.017039349302649498\n",
            "Epoch 1, Batch 10, Loss: 0.015990260988473892\n",
            "Epoch 1, Batch 20, Loss: 0.025711046531796455\n",
            "Epoch 1, Batch 30, Loss: 0.020296484231948853\n",
            "Epoch 1, Batch 40, Loss: 0.019102105870842934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([42, 1])) that is different to the input size (torch.Size([42, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 50, Loss: 0.02472699247300625\n",
            "Epoch 1, Batch 60, Loss: 0.024567048996686935\n",
            "Epoch 1, Batch 70, Loss: 0.024860592558979988\n",
            "Epoch 1, Batch 80, Loss: 0.025089504197239876\n",
            "Epoch 1, Batch 90, Loss: 0.01617947220802307\n",
            "Epoch 1, Batch 100, Loss: 0.019884582608938217\n",
            "Epoch 1, Batch 110, Loss: 0.02272436022758484\n",
            "Epoch 1, Batch 120, Loss: 0.016819240525364876\n",
            "Epoch 1, Batch 130, Loss: 0.017689619213342667\n",
            "Epoch 1, Batch 140, Loss: 0.01152730081230402\n",
            "Epoch 1, Batch 150, Loss: 0.01621892675757408\n",
            "Epoch 1, Batch 160, Loss: 0.017710402607917786\n",
            "Epoch 1, Batch 170, Loss: 0.01603238843381405\n",
            "Epoch 1, Batch 180, Loss: 0.01740460842847824\n",
            "Epoch 1, Batch 190, Loss: 0.02030702866613865\n",
            "Epoch 1, Batch 200, Loss: 0.01678253337740898\n",
            "Epoch 1, Batch 210, Loss: 0.019896535202860832\n",
            "Epoch 1, Batch 220, Loss: 0.016224846243858337\n",
            "Epoch 1, Batch 230, Loss: 0.02499503456056118\n",
            "Epoch 1, Batch 240, Loss: 0.02150282822549343\n",
            "Epoch 1, Batch 250, Loss: 0.020903846248984337\n",
            "Epoch 1, Batch 260, Loss: 0.013533935882151127\n",
            "Epoch 1, Batch 270, Loss: 0.01923145353794098\n",
            "Epoch 1, Batch 280, Loss: 0.016796808689832687\n",
            "Epoch 1, Batch 290, Loss: 0.013958288356661797\n",
            "Epoch 1, Batch 300, Loss: 0.0198139026761055\n",
            "Epoch 1, Batch 310, Loss: 0.024253973737359047\n",
            "Epoch 1, Batch 320, Loss: 0.021446919068694115\n",
            "Epoch 1, Batch 330, Loss: 0.01676790602505207\n",
            "Epoch 1, Batch 340, Loss: 0.011825007386505604\n",
            "Epoch 1, Batch 350, Loss: 0.01375384908169508\n",
            "Epoch 1, Batch 360, Loss: 0.015188075602054596\n",
            "Epoch 1, Batch 370, Loss: 0.01996021345257759\n",
            "Epoch 1, Batch 380, Loss: 0.022914404049515724\n",
            "Epoch 1, Batch 390, Loss: 0.024046987295150757\n",
            "Epoch 1, Batch 400, Loss: 0.020192958414554596\n",
            "Epoch 1, Batch 410, Loss: 0.017884226515889168\n",
            "Epoch 1, Batch 420, Loss: 0.024060530588030815\n",
            "Epoch 1, Batch 430, Loss: 0.018191969022154808\n",
            "Epoch 1, Batch 440, Loss: 0.014731505885720253\n",
            "Epoch 1, Batch 450, Loss: 0.022017888724803925\n",
            "Epoch 1, Batch 460, Loss: 0.01818273589015007\n",
            "Epoch 1, Batch 470, Loss: 0.012780207209289074\n",
            "Epoch 1, Batch 480, Loss: 0.021484309807419777\n",
            "Epoch 1, Batch 490, Loss: 0.015777818858623505\n",
            "Epoch 1, Batch 500, Loss: 0.01921495422720909\n",
            "Epoch 1, Batch 510, Loss: 0.020347097888588905\n",
            "Epoch 1, Batch 520, Loss: 0.018610190600156784\n",
            "Epoch 1, Batch 530, Loss: 0.019085561856627464\n",
            "Epoch 1, Batch 540, Loss: 0.027045488357543945\n",
            "Epoch 1, Batch 550, Loss: 0.010364002548158169\n",
            "Epoch 1, Batch 560, Loss: 0.011489944532513618\n",
            "Epoch 1, Batch 570, Loss: 0.02185426838696003\n",
            "Epoch 1, Batch 580, Loss: 0.01280917041003704\n",
            "Epoch 1, Batch 590, Loss: 0.017193857580423355\n",
            "Epoch 1, Batch 600, Loss: 0.014275984838604927\n",
            "Epoch 1, Batch 610, Loss: 0.020246004685759544\n",
            "Epoch 1, Batch 620, Loss: 0.014306578785181046\n",
            "Epoch 1, Batch 630, Loss: 0.013706421479582787\n",
            "Epoch 1, Batch 640, Loss: 0.016411192715168\n",
            "Epoch 1, Batch 650, Loss: 0.013470213860273361\n",
            "Epoch 1, Batch 660, Loss: 0.015142064541578293\n",
            "Epoch 1, Batch 670, Loss: 0.017326293513178825\n",
            "Epoch 1, Batch 680, Loss: 0.01555287279188633\n",
            "Epoch 1, Batch 690, Loss: 0.015800366178154945\n",
            "Epoch 1, Batch 700, Loss: 0.01603679172694683\n",
            "Epoch 1, Batch 710, Loss: 0.018729548901319504\n",
            "Epoch 2, Batch 0, Loss: 0.016731275245547295\n",
            "Epoch 2, Batch 10, Loss: 0.013759666122496128\n",
            "Epoch 2, Batch 20, Loss: 0.017865879461169243\n",
            "Epoch 2, Batch 30, Loss: 0.014133642427623272\n",
            "Epoch 2, Batch 40, Loss: 0.014637739397585392\n",
            "Epoch 2, Batch 50, Loss: 0.01280122995376587\n",
            "Epoch 2, Batch 60, Loss: 0.015998929738998413\n",
            "Epoch 2, Batch 70, Loss: 0.00948367454111576\n",
            "Epoch 2, Batch 80, Loss: 0.01184212788939476\n",
            "Epoch 2, Batch 90, Loss: 0.01323116384446621\n",
            "Epoch 2, Batch 100, Loss: 0.021791350096464157\n",
            "Epoch 2, Batch 110, Loss: 0.019339054822921753\n",
            "Epoch 2, Batch 120, Loss: 0.01209243293851614\n",
            "Epoch 2, Batch 130, Loss: 0.014404699206352234\n",
            "Epoch 2, Batch 140, Loss: 0.023792104795575142\n",
            "Epoch 2, Batch 150, Loss: 0.019863003864884377\n",
            "Epoch 2, Batch 160, Loss: 0.019265230745077133\n",
            "Epoch 2, Batch 170, Loss: 0.008780757896602154\n",
            "Epoch 2, Batch 180, Loss: 0.013564523309469223\n",
            "Epoch 2, Batch 190, Loss: 0.010181456804275513\n",
            "Epoch 2, Batch 200, Loss: 0.014456006698310375\n",
            "Epoch 2, Batch 210, Loss: 0.011895915493369102\n",
            "Epoch 2, Batch 220, Loss: 0.011645922437310219\n",
            "Epoch 2, Batch 230, Loss: 0.014546734280884266\n",
            "Epoch 2, Batch 240, Loss: 0.01774396002292633\n",
            "Epoch 2, Batch 250, Loss: 0.018628869205713272\n",
            "Epoch 2, Batch 260, Loss: 0.016133921220898628\n",
            "Epoch 2, Batch 270, Loss: 0.014848021790385246\n",
            "Epoch 2, Batch 280, Loss: 0.015454265289008617\n",
            "Epoch 2, Batch 290, Loss: 0.013428945094347\n",
            "Epoch 2, Batch 300, Loss: 0.012642557732760906\n",
            "Epoch 2, Batch 310, Loss: 0.011958738788962364\n",
            "Epoch 2, Batch 320, Loss: 0.014536936767399311\n",
            "Epoch 2, Batch 330, Loss: 0.014863570220768452\n",
            "Epoch 2, Batch 340, Loss: 0.017476219683885574\n",
            "Epoch 2, Batch 350, Loss: 0.025667186826467514\n",
            "Epoch 2, Batch 360, Loss: 0.016802875325083733\n",
            "Epoch 2, Batch 370, Loss: 0.0232158824801445\n",
            "Epoch 2, Batch 380, Loss: 0.016464801505208015\n",
            "Epoch 2, Batch 390, Loss: 0.014237131923437119\n",
            "Epoch 2, Batch 400, Loss: 0.015707168728113174\n",
            "Epoch 2, Batch 410, Loss: 0.01719656027853489\n",
            "Epoch 2, Batch 420, Loss: 0.011147640645503998\n",
            "Epoch 2, Batch 430, Loss: 0.013886265456676483\n",
            "Epoch 2, Batch 440, Loss: 0.01455809734761715\n",
            "Epoch 2, Batch 450, Loss: 0.010460530407726765\n",
            "Epoch 2, Batch 460, Loss: 0.015809908509254456\n",
            "Epoch 2, Batch 470, Loss: 0.018227502703666687\n",
            "Epoch 2, Batch 480, Loss: 0.01420851331204176\n",
            "Epoch 2, Batch 490, Loss: 0.017717326059937477\n",
            "Epoch 2, Batch 500, Loss: 0.01786467619240284\n",
            "Epoch 2, Batch 510, Loss: 0.019904905930161476\n",
            "Epoch 2, Batch 520, Loss: 0.015753179788589478\n",
            "Epoch 2, Batch 530, Loss: 0.013901730068027973\n",
            "Epoch 2, Batch 540, Loss: 0.014315717853605747\n",
            "Epoch 2, Batch 550, Loss: 0.010960813611745834\n",
            "Epoch 2, Batch 560, Loss: 0.014150727540254593\n",
            "Epoch 2, Batch 570, Loss: 0.012814153917133808\n",
            "Epoch 2, Batch 580, Loss: 0.01483326405286789\n",
            "Epoch 2, Batch 590, Loss: 0.01256539300084114\n",
            "Epoch 2, Batch 600, Loss: 0.010665830224752426\n",
            "Epoch 2, Batch 610, Loss: 0.011612724512815475\n",
            "Epoch 2, Batch 620, Loss: 0.012423698790371418\n",
            "Epoch 2, Batch 630, Loss: 0.01458834856748581\n",
            "Epoch 2, Batch 640, Loss: 0.016847265884280205\n",
            "Epoch 2, Batch 650, Loss: 0.02139130048453808\n",
            "Epoch 2, Batch 660, Loss: 0.016313813626766205\n",
            "Epoch 2, Batch 670, Loss: 0.01378295011818409\n",
            "Epoch 2, Batch 680, Loss: 0.015817081555724144\n",
            "Epoch 2, Batch 690, Loss: 0.01234572846442461\n",
            "Epoch 2, Batch 700, Loss: 0.01860908232629299\n",
            "Epoch 2, Batch 710, Loss: 0.010420321486890316\n",
            "Epoch 3, Batch 0, Loss: 0.015976591035723686\n",
            "Epoch 3, Batch 10, Loss: 0.014764934778213501\n",
            "Epoch 3, Batch 20, Loss: 0.013546557165682316\n",
            "Epoch 3, Batch 30, Loss: 0.01524175051599741\n",
            "Epoch 3, Batch 40, Loss: 0.011640782468020916\n",
            "Epoch 3, Batch 50, Loss: 0.012961109168827534\n",
            "Epoch 3, Batch 60, Loss: 0.011427445337176323\n",
            "Epoch 3, Batch 70, Loss: 0.011862128041684628\n",
            "Epoch 3, Batch 80, Loss: 0.012303055264055729\n",
            "Epoch 3, Batch 90, Loss: 0.015289031900465488\n",
            "Epoch 3, Batch 100, Loss: 0.018453560769557953\n",
            "Epoch 3, Batch 110, Loss: 0.016248267143964767\n",
            "Epoch 3, Batch 120, Loss: 0.01026241946965456\n",
            "Epoch 3, Batch 130, Loss: 0.014052952639758587\n",
            "Epoch 3, Batch 140, Loss: 0.016916515305638313\n",
            "Epoch 3, Batch 150, Loss: 0.009290358982980251\n",
            "Epoch 3, Batch 160, Loss: 0.014278467744588852\n",
            "Epoch 3, Batch 170, Loss: 0.01779007725417614\n",
            "Epoch 3, Batch 180, Loss: 0.008130194619297981\n",
            "Epoch 3, Batch 190, Loss: 0.012615269050002098\n",
            "Epoch 3, Batch 200, Loss: 0.01451901439577341\n",
            "Epoch 3, Batch 210, Loss: 0.013922697864472866\n",
            "Epoch 3, Batch 220, Loss: 0.014552222564816475\n",
            "Epoch 3, Batch 230, Loss: 0.016840236261487007\n",
            "Epoch 3, Batch 240, Loss: 0.011185036972165108\n",
            "Epoch 3, Batch 250, Loss: 0.013814994134008884\n",
            "Epoch 3, Batch 260, Loss: 0.008503003045916557\n",
            "Epoch 3, Batch 270, Loss: 0.007850282825529575\n",
            "Epoch 3, Batch 280, Loss: 0.013767220079898834\n",
            "Epoch 3, Batch 290, Loss: 0.012566628865897655\n",
            "Epoch 3, Batch 300, Loss: 0.013881939463317394\n",
            "Epoch 3, Batch 310, Loss: 0.014531943947076797\n",
            "Epoch 3, Batch 320, Loss: 0.01078200712800026\n",
            "Epoch 3, Batch 330, Loss: 0.012695442885160446\n",
            "Epoch 3, Batch 340, Loss: 0.014997138641774654\n",
            "Epoch 3, Batch 350, Loss: 0.012657292187213898\n",
            "Epoch 3, Batch 360, Loss: 0.021878551691770554\n",
            "Epoch 3, Batch 370, Loss: 0.013721800409257412\n",
            "Epoch 3, Batch 380, Loss: 0.016555026173591614\n",
            "Epoch 3, Batch 390, Loss: 0.016456149518489838\n",
            "Epoch 3, Batch 400, Loss: 0.010079344734549522\n",
            "Epoch 3, Batch 410, Loss: 0.008847298100590706\n",
            "Epoch 3, Batch 420, Loss: 0.012036817148327827\n",
            "Epoch 3, Batch 430, Loss: 0.013125256635248661\n",
            "Epoch 3, Batch 440, Loss: 0.016078246757388115\n",
            "Epoch 3, Batch 450, Loss: 0.011573141440749168\n",
            "Epoch 3, Batch 460, Loss: 0.016587860882282257\n",
            "Epoch 3, Batch 470, Loss: 0.012411226518452168\n",
            "Epoch 3, Batch 480, Loss: 0.013840027153491974\n",
            "Epoch 3, Batch 490, Loss: 0.011979347094893456\n",
            "Epoch 3, Batch 500, Loss: 0.012650800868868828\n",
            "Epoch 3, Batch 510, Loss: 0.014109612442553043\n",
            "Epoch 3, Batch 520, Loss: 0.012829550541937351\n",
            "Epoch 3, Batch 530, Loss: 0.011155214160680771\n",
            "Epoch 3, Batch 540, Loss: 0.01312220934778452\n",
            "Epoch 3, Batch 550, Loss: 0.011443383060395718\n",
            "Epoch 3, Batch 560, Loss: 0.012524108402431011\n",
            "Epoch 3, Batch 570, Loss: 0.014475547708570957\n",
            "Epoch 3, Batch 580, Loss: 0.01561293937265873\n",
            "Epoch 3, Batch 590, Loss: 0.014736839570105076\n",
            "Epoch 3, Batch 600, Loss: 0.01819342002272606\n",
            "Epoch 3, Batch 610, Loss: 0.01298521738499403\n",
            "Epoch 3, Batch 620, Loss: 0.017062533646821976\n",
            "Epoch 3, Batch 630, Loss: 0.01664237305521965\n",
            "Epoch 3, Batch 640, Loss: 0.012873142957687378\n",
            "Epoch 3, Batch 650, Loss: 0.013055730611085892\n",
            "Epoch 3, Batch 660, Loss: 0.01284711342304945\n",
            "Epoch 3, Batch 670, Loss: 0.013699319213628769\n",
            "Epoch 3, Batch 680, Loss: 0.013266453519463539\n",
            "Epoch 3, Batch 690, Loss: 0.015955552458763123\n",
            "Epoch 3, Batch 700, Loss: 0.014514660462737083\n",
            "Epoch 3, Batch 710, Loss: 0.016163460910320282\n",
            "Epoch 4, Batch 0, Loss: 0.014833318069577217\n",
            "Epoch 4, Batch 10, Loss: 0.015630995854735374\n",
            "Epoch 4, Batch 20, Loss: 0.0201487485319376\n",
            "Epoch 4, Batch 30, Loss: 0.012084223330020905\n",
            "Epoch 4, Batch 40, Loss: 0.014746461063623428\n",
            "Epoch 4, Batch 50, Loss: 0.018008621409535408\n",
            "Epoch 4, Batch 60, Loss: 0.01230090856552124\n",
            "Epoch 4, Batch 70, Loss: 0.008375884033739567\n",
            "Epoch 4, Batch 80, Loss: 0.022242339327931404\n",
            "Epoch 4, Batch 90, Loss: 0.011213092133402824\n",
            "Epoch 4, Batch 100, Loss: 0.013262072578072548\n",
            "Epoch 4, Batch 110, Loss: 0.012769186869263649\n",
            "Epoch 4, Batch 120, Loss: 0.013882889412343502\n",
            "Epoch 4, Batch 130, Loss: 0.015058823861181736\n",
            "Epoch 4, Batch 140, Loss: 0.01882820576429367\n",
            "Epoch 4, Batch 150, Loss: 0.008840409107506275\n",
            "Epoch 4, Batch 160, Loss: 0.010051276534795761\n",
            "Epoch 4, Batch 170, Loss: 0.014362177811563015\n",
            "Epoch 4, Batch 180, Loss: 0.016999030485749245\n",
            "Epoch 4, Batch 190, Loss: 0.013733902014791965\n",
            "Epoch 4, Batch 200, Loss: 0.013751265592873096\n",
            "Epoch 4, Batch 210, Loss: 0.010221682488918304\n",
            "Epoch 4, Batch 220, Loss: 0.010859182104468346\n",
            "Epoch 4, Batch 230, Loss: 0.020785091444849968\n",
            "Epoch 4, Batch 240, Loss: 0.01570367068052292\n",
            "Epoch 4, Batch 250, Loss: 0.01576964557170868\n",
            "Epoch 4, Batch 260, Loss: 0.011412850581109524\n",
            "Epoch 4, Batch 270, Loss: 0.012320580892264843\n",
            "Epoch 4, Batch 280, Loss: 0.015213808976113796\n",
            "Epoch 4, Batch 290, Loss: 0.009240676648914814\n",
            "Epoch 4, Batch 300, Loss: 0.016986634582281113\n",
            "Epoch 4, Batch 310, Loss: 0.017026104032993317\n",
            "Epoch 4, Batch 320, Loss: 0.01338245440274477\n",
            "Epoch 4, Batch 330, Loss: 0.011485600844025612\n",
            "Epoch 4, Batch 340, Loss: 0.010220653377473354\n",
            "Epoch 4, Batch 350, Loss: 0.014227156527340412\n",
            "Epoch 4, Batch 360, Loss: 0.011363300494849682\n",
            "Epoch 4, Batch 370, Loss: 0.02009424939751625\n",
            "Epoch 4, Batch 380, Loss: 0.010296156629920006\n",
            "Epoch 4, Batch 390, Loss: 0.01678185909986496\n",
            "Epoch 4, Batch 400, Loss: 0.014547539874911308\n",
            "Epoch 4, Batch 410, Loss: 0.016274765133857727\n",
            "Epoch 4, Batch 420, Loss: 0.01567831262946129\n",
            "Epoch 4, Batch 430, Loss: 0.014387628063559532\n",
            "Epoch 4, Batch 440, Loss: 0.02189142443239689\n",
            "Epoch 4, Batch 450, Loss: 0.011580074205994606\n",
            "Epoch 4, Batch 460, Loss: 0.011004606261849403\n",
            "Epoch 4, Batch 470, Loss: 0.012996411882340908\n",
            "Epoch 4, Batch 480, Loss: 0.015294882468879223\n",
            "Epoch 4, Batch 490, Loss: 0.010477087460458279\n",
            "Epoch 4, Batch 500, Loss: 0.01328469067811966\n",
            "Epoch 4, Batch 510, Loss: 0.01426665298640728\n",
            "Epoch 4, Batch 520, Loss: 0.01594747044146061\n",
            "Epoch 4, Batch 530, Loss: 0.013116116635501385\n",
            "Epoch 4, Batch 540, Loss: 0.016917942091822624\n",
            "Epoch 4, Batch 550, Loss: 0.013177813962101936\n",
            "Epoch 4, Batch 560, Loss: 0.008880116045475006\n",
            "Epoch 4, Batch 570, Loss: 0.014214973896741867\n",
            "Epoch 4, Batch 580, Loss: 0.012372060678899288\n",
            "Epoch 4, Batch 590, Loss: 0.012234019115567207\n",
            "Epoch 4, Batch 600, Loss: 0.014429094269871712\n",
            "Epoch 4, Batch 610, Loss: 0.01053246296942234\n",
            "Epoch 4, Batch 620, Loss: 0.014447666704654694\n",
            "Epoch 4, Batch 630, Loss: 0.014156515710055828\n",
            "Epoch 4, Batch 640, Loss: 0.014562496915459633\n",
            "Epoch 4, Batch 650, Loss: 0.013622322119772434\n",
            "Epoch 4, Batch 660, Loss: 0.010661155916750431\n",
            "Epoch 4, Batch 670, Loss: 0.011455897241830826\n",
            "Epoch 4, Batch 680, Loss: 0.01525312289595604\n",
            "Epoch 4, Batch 690, Loss: 0.017084475606679916\n",
            "Epoch 4, Batch 700, Loss: 0.011800884269177914\n",
            "Epoch 4, Batch 710, Loss: 0.01355464942753315\n",
            "Epoch 5, Batch 0, Loss: 0.009444480761885643\n",
            "Epoch 5, Batch 10, Loss: 0.01555068138986826\n",
            "Epoch 5, Batch 20, Loss: 0.007993553765118122\n",
            "Epoch 5, Batch 30, Loss: 0.0122756976634264\n",
            "Epoch 5, Batch 40, Loss: 0.010748136788606644\n",
            "Epoch 5, Batch 50, Loss: 0.01474926620721817\n",
            "Epoch 5, Batch 60, Loss: 0.012590491212904453\n",
            "Epoch 5, Batch 70, Loss: 0.01374188344925642\n",
            "Epoch 5, Batch 80, Loss: 0.011749251745641232\n",
            "Epoch 5, Batch 90, Loss: 0.010534944012761116\n",
            "Epoch 5, Batch 100, Loss: 0.011385749094188213\n",
            "Epoch 5, Batch 110, Loss: 0.012205525301396847\n",
            "Epoch 5, Batch 120, Loss: 0.010217124596238136\n",
            "Epoch 5, Batch 130, Loss: 0.012027758173644543\n",
            "Epoch 5, Batch 140, Loss: 0.011227733455598354\n",
            "Epoch 5, Batch 150, Loss: 0.011072967201471329\n",
            "Epoch 5, Batch 160, Loss: 0.013346746563911438\n",
            "Epoch 5, Batch 170, Loss: 0.01122805941849947\n",
            "Epoch 5, Batch 180, Loss: 0.01373473834246397\n",
            "Epoch 5, Batch 190, Loss: 0.018365517258644104\n",
            "Epoch 5, Batch 200, Loss: 0.011659776791930199\n",
            "Epoch 5, Batch 210, Loss: 0.01262873038649559\n",
            "Epoch 5, Batch 220, Loss: 0.013922447338700294\n",
            "Epoch 5, Batch 230, Loss: 0.014873530715703964\n",
            "Epoch 5, Batch 240, Loss: 0.013790843077003956\n",
            "Epoch 5, Batch 250, Loss: 0.01735163852572441\n",
            "Epoch 5, Batch 260, Loss: 0.009438845328986645\n",
            "Epoch 5, Batch 270, Loss: 0.010182403028011322\n",
            "Epoch 5, Batch 280, Loss: 0.015992481261491776\n",
            "Epoch 5, Batch 290, Loss: 0.011658678762614727\n",
            "Epoch 5, Batch 300, Loss: 0.014822282828390598\n",
            "Epoch 5, Batch 310, Loss: 0.019093846902251244\n",
            "Epoch 5, Batch 320, Loss: 0.010747913271188736\n",
            "Epoch 5, Batch 330, Loss: 0.012221117503941059\n",
            "Epoch 5, Batch 340, Loss: 0.012696291320025921\n",
            "Epoch 5, Batch 350, Loss: 0.010359921492636204\n",
            "Epoch 5, Batch 360, Loss: 0.017476236447691917\n",
            "Epoch 5, Batch 370, Loss: 0.014689336530864239\n",
            "Epoch 5, Batch 380, Loss: 0.014201199635863304\n",
            "Epoch 5, Batch 390, Loss: 0.013757315464317799\n",
            "Epoch 5, Batch 400, Loss: 0.010042809881269932\n",
            "Epoch 5, Batch 410, Loss: 0.018559040501713753\n",
            "Epoch 5, Batch 420, Loss: 0.01223283726722002\n",
            "Epoch 5, Batch 430, Loss: 0.012550534680485725\n",
            "Epoch 5, Batch 440, Loss: 0.020352885127067566\n",
            "Epoch 5, Batch 450, Loss: 0.012976197525858879\n",
            "Epoch 5, Batch 460, Loss: 0.012407388538122177\n",
            "Epoch 5, Batch 470, Loss: 0.013770666904747486\n",
            "Epoch 5, Batch 480, Loss: 0.012518358416855335\n",
            "Epoch 5, Batch 490, Loss: 0.010955213569104671\n",
            "Epoch 5, Batch 500, Loss: 0.012049175798892975\n",
            "Epoch 5, Batch 510, Loss: 0.011144421994686127\n",
            "Epoch 5, Batch 520, Loss: 0.015971779823303223\n",
            "Epoch 5, Batch 530, Loss: 0.012740951031446457\n",
            "Epoch 5, Batch 540, Loss: 0.018169518560171127\n",
            "Epoch 5, Batch 550, Loss: 0.010995093733072281\n",
            "Epoch 5, Batch 560, Loss: 0.007444794289767742\n",
            "Epoch 5, Batch 570, Loss: 0.016127418726682663\n",
            "Epoch 5, Batch 580, Loss: 0.026492541655898094\n",
            "Epoch 5, Batch 590, Loss: 0.011505939066410065\n",
            "Epoch 5, Batch 600, Loss: 0.012609759345650673\n",
            "Epoch 5, Batch 610, Loss: 0.01417861133813858\n",
            "Epoch 5, Batch 620, Loss: 0.013949270360171795\n",
            "Epoch 5, Batch 630, Loss: 0.016805501654744148\n",
            "Epoch 5, Batch 640, Loss: 0.016002340242266655\n",
            "Epoch 5, Batch 650, Loss: 0.015553000383079052\n",
            "Epoch 5, Batch 660, Loss: 0.012773685157299042\n",
            "Epoch 5, Batch 670, Loss: 0.01379997469484806\n",
            "Epoch 5, Batch 680, Loss: 0.015193386003375053\n",
            "Epoch 5, Batch 690, Loss: 0.014834558591246605\n",
            "Epoch 5, Batch 700, Loss: 0.012761635705828667\n",
            "Epoch 5, Batch 710, Loss: 0.01634414680302143\n",
            "Epoch 6, Batch 0, Loss: 0.013253123499453068\n",
            "Epoch 6, Batch 10, Loss: 0.01050560176372528\n",
            "Epoch 6, Batch 20, Loss: 0.013234796933829784\n",
            "Epoch 6, Batch 30, Loss: 0.010962259955704212\n",
            "Epoch 6, Batch 40, Loss: 0.009335421025753021\n",
            "Epoch 6, Batch 50, Loss: 0.012221893295645714\n",
            "Epoch 6, Batch 60, Loss: 0.011236622929573059\n",
            "Epoch 6, Batch 70, Loss: 0.012486702762544155\n",
            "Epoch 6, Batch 80, Loss: 0.010840773582458496\n",
            "Epoch 6, Batch 90, Loss: 0.00986213143914938\n",
            "Epoch 6, Batch 100, Loss: 0.014664262533187866\n",
            "Epoch 6, Batch 110, Loss: 0.010079175233840942\n",
            "Epoch 6, Batch 120, Loss: 0.00900616217404604\n",
            "Epoch 6, Batch 130, Loss: 0.010108943097293377\n",
            "Epoch 6, Batch 140, Loss: 0.009013688191771507\n",
            "Epoch 6, Batch 150, Loss: 0.011387594044208527\n",
            "Epoch 6, Batch 160, Loss: 0.009926044382154942\n",
            "Epoch 6, Batch 170, Loss: 0.013824385590851307\n",
            "Epoch 6, Batch 180, Loss: 0.008928666822612286\n",
            "Epoch 6, Batch 190, Loss: 0.01229822263121605\n",
            "Epoch 6, Batch 200, Loss: 0.018692096695303917\n",
            "Epoch 6, Batch 210, Loss: 0.00854920968413353\n",
            "Epoch 6, Batch 220, Loss: 0.014511357992887497\n",
            "Epoch 6, Batch 230, Loss: 0.00791502557694912\n",
            "Epoch 6, Batch 240, Loss: 0.011202249675989151\n",
            "Epoch 6, Batch 250, Loss: 0.011106076650321484\n",
            "Epoch 6, Batch 260, Loss: 0.012713433243334293\n",
            "Epoch 6, Batch 270, Loss: 0.009570964612066746\n",
            "Epoch 6, Batch 280, Loss: 0.013678440824151039\n",
            "Epoch 6, Batch 290, Loss: 0.011889692395925522\n",
            "Epoch 6, Batch 300, Loss: 0.012126056477427483\n",
            "Epoch 6, Batch 310, Loss: 0.011841249652206898\n",
            "Epoch 6, Batch 320, Loss: 0.012133155949413776\n",
            "Epoch 6, Batch 330, Loss: 0.009949474595487118\n",
            "Epoch 6, Batch 340, Loss: 0.011745382100343704\n",
            "Epoch 6, Batch 350, Loss: 0.008988295681774616\n",
            "Epoch 6, Batch 360, Loss: 0.01636345125734806\n",
            "Epoch 6, Batch 370, Loss: 0.009745876304805279\n",
            "Epoch 6, Batch 380, Loss: 0.01972169429063797\n",
            "Epoch 6, Batch 390, Loss: 0.011276726610958576\n",
            "Epoch 6, Batch 400, Loss: 0.011701902374625206\n",
            "Epoch 6, Batch 410, Loss: 0.010760543867945671\n",
            "Epoch 6, Batch 420, Loss: 0.012522201053798199\n",
            "Epoch 6, Batch 430, Loss: 0.013343165628612041\n",
            "Epoch 6, Batch 440, Loss: 0.014796028845012188\n",
            "Epoch 6, Batch 450, Loss: 0.01743537001311779\n",
            "Epoch 6, Batch 460, Loss: 0.009814368560910225\n",
            "Epoch 6, Batch 470, Loss: 0.017721114680171013\n",
            "Epoch 6, Batch 480, Loss: 0.01334598008543253\n",
            "Epoch 6, Batch 490, Loss: 0.009677953086793423\n",
            "Epoch 6, Batch 500, Loss: 0.009819907136261463\n",
            "Epoch 6, Batch 510, Loss: 0.020016854628920555\n",
            "Epoch 6, Batch 520, Loss: 0.01105685718357563\n",
            "Epoch 6, Batch 530, Loss: 0.01704038307070732\n",
            "Epoch 6, Batch 540, Loss: 0.009407499805092812\n",
            "Epoch 6, Batch 550, Loss: 0.016229048371315002\n",
            "Epoch 6, Batch 560, Loss: 0.015867935493588448\n",
            "Epoch 6, Batch 570, Loss: 0.011321460828185081\n",
            "Epoch 6, Batch 580, Loss: 0.016853803768754005\n",
            "Epoch 6, Batch 590, Loss: 0.016372527927160263\n",
            "Epoch 6, Batch 600, Loss: 0.011727621778845787\n",
            "Epoch 6, Batch 610, Loss: 0.01208775956183672\n",
            "Epoch 6, Batch 620, Loss: 0.015023985877633095\n",
            "Epoch 6, Batch 630, Loss: 0.010144803673028946\n",
            "Epoch 6, Batch 640, Loss: 0.01825666055083275\n",
            "Epoch 6, Batch 650, Loss: 0.011860596016049385\n",
            "Epoch 6, Batch 660, Loss: 0.011429804377257824\n",
            "Epoch 6, Batch 670, Loss: 0.013733016327023506\n",
            "Epoch 6, Batch 680, Loss: 0.011824377812445164\n",
            "Epoch 6, Batch 690, Loss: 0.011575136333703995\n",
            "Epoch 6, Batch 700, Loss: 0.012431468814611435\n",
            "Epoch 6, Batch 710, Loss: 0.008078210987150669\n",
            "Epoch 7, Batch 0, Loss: 0.010890735313296318\n",
            "Epoch 7, Batch 10, Loss: 0.012819167226552963\n",
            "Epoch 7, Batch 20, Loss: 0.010685001499950886\n",
            "Epoch 7, Batch 30, Loss: 0.012575209140777588\n",
            "Epoch 7, Batch 40, Loss: 0.010873733088374138\n",
            "Epoch 7, Batch 50, Loss: 0.007972801104187965\n",
            "Epoch 7, Batch 60, Loss: 0.008259676396846771\n",
            "Epoch 7, Batch 70, Loss: 0.009180043824017048\n",
            "Epoch 7, Batch 80, Loss: 0.007162655238062143\n",
            "Epoch 7, Batch 90, Loss: 0.00931877363473177\n",
            "Epoch 7, Batch 100, Loss: 0.012361554428935051\n",
            "Epoch 7, Batch 110, Loss: 0.010786214843392372\n",
            "Epoch 7, Batch 120, Loss: 0.009850426577031612\n",
            "Epoch 7, Batch 130, Loss: 0.010360952466726303\n",
            "Epoch 7, Batch 140, Loss: 0.00994919054210186\n",
            "Epoch 7, Batch 150, Loss: 0.009705517441034317\n",
            "Epoch 7, Batch 160, Loss: 0.010348377749323845\n",
            "Epoch 7, Batch 170, Loss: 0.007775382604449987\n",
            "Epoch 7, Batch 180, Loss: 0.011340497061610222\n",
            "Epoch 7, Batch 190, Loss: 0.009260566905140877\n",
            "Epoch 7, Batch 200, Loss: 0.011986639350652695\n",
            "Epoch 7, Batch 210, Loss: 0.011606455780565739\n",
            "Epoch 7, Batch 220, Loss: 0.01553928293287754\n",
            "Epoch 7, Batch 230, Loss: 0.013439368456602097\n",
            "Epoch 7, Batch 240, Loss: 0.016407286748290062\n",
            "Epoch 7, Batch 250, Loss: 0.013593412935733795\n",
            "Epoch 7, Batch 260, Loss: 0.012221570126712322\n",
            "Epoch 7, Batch 270, Loss: 0.010341636836528778\n",
            "Epoch 7, Batch 280, Loss: 0.01058873813599348\n",
            "Epoch 7, Batch 290, Loss: 0.009324591606855392\n",
            "Epoch 7, Batch 300, Loss: 0.011707257479429245\n",
            "Epoch 7, Batch 310, Loss: 0.009020313620567322\n",
            "Epoch 7, Batch 320, Loss: 0.013262679800391197\n",
            "Epoch 7, Batch 330, Loss: 0.007677971385419369\n",
            "Epoch 7, Batch 340, Loss: 0.008871018886566162\n",
            "Epoch 7, Batch 350, Loss: 0.015378393232822418\n",
            "Epoch 7, Batch 360, Loss: 0.01700921542942524\n",
            "Epoch 7, Batch 370, Loss: 0.013045341707766056\n",
            "Epoch 7, Batch 380, Loss: 0.012965966947376728\n",
            "Epoch 7, Batch 390, Loss: 0.00981852412223816\n",
            "Epoch 7, Batch 400, Loss: 0.013835610821843147\n",
            "Epoch 7, Batch 410, Loss: 0.011018176563084126\n",
            "Epoch 7, Batch 420, Loss: 0.013715825043618679\n",
            "Epoch 7, Batch 430, Loss: 0.011297157034277916\n",
            "Epoch 7, Batch 440, Loss: 0.016672257333993912\n",
            "Epoch 7, Batch 450, Loss: 0.010580100119113922\n",
            "Epoch 7, Batch 460, Loss: 0.010553166270256042\n",
            "Epoch 7, Batch 470, Loss: 0.014092689380049706\n",
            "Epoch 7, Batch 480, Loss: 0.008903971873223782\n",
            "Epoch 7, Batch 490, Loss: 0.0065412684343755245\n",
            "Epoch 7, Batch 500, Loss: 0.01617974229156971\n",
            "Epoch 7, Batch 510, Loss: 0.00881115347146988\n",
            "Epoch 7, Batch 520, Loss: 0.012845885008573532\n",
            "Epoch 7, Batch 530, Loss: 0.01262748334556818\n",
            "Epoch 7, Batch 540, Loss: 0.012944778427481651\n",
            "Epoch 7, Batch 550, Loss: 0.015524735674262047\n",
            "Epoch 7, Batch 560, Loss: 0.008800090290606022\n",
            "Epoch 7, Batch 570, Loss: 0.011517148464918137\n",
            "Epoch 7, Batch 580, Loss: 0.011490408331155777\n",
            "Epoch 7, Batch 590, Loss: 0.011512236669659615\n",
            "Epoch 7, Batch 600, Loss: 0.013176687061786652\n",
            "Epoch 7, Batch 610, Loss: 0.015963243320584297\n",
            "Epoch 7, Batch 620, Loss: 0.010935487225651741\n",
            "Epoch 7, Batch 630, Loss: 0.011189705692231655\n",
            "Epoch 7, Batch 640, Loss: 0.010593981482088566\n",
            "Epoch 7, Batch 650, Loss: 0.009629795327782631\n",
            "Epoch 7, Batch 660, Loss: 0.013045892119407654\n",
            "Epoch 7, Batch 670, Loss: 0.012624640017747879\n",
            "Epoch 7, Batch 680, Loss: 0.010316724888980389\n",
            "Epoch 7, Batch 690, Loss: 0.008913125842809677\n",
            "Epoch 7, Batch 700, Loss: 0.015400087460875511\n",
            "Epoch 7, Batch 710, Loss: 0.009995224885642529\n",
            "Epoch 8, Batch 0, Loss: 0.011347205378115177\n",
            "Epoch 8, Batch 10, Loss: 0.010553465224802494\n",
            "Epoch 8, Batch 20, Loss: 0.028140312060713768\n",
            "Epoch 8, Batch 30, Loss: 0.015021027065813541\n",
            "Epoch 8, Batch 40, Loss: 0.011548773385584354\n",
            "Epoch 8, Batch 50, Loss: 0.014717485755681992\n",
            "Epoch 8, Batch 60, Loss: 0.013170739635825157\n",
            "Epoch 8, Batch 70, Loss: 0.01150500401854515\n",
            "Epoch 8, Batch 80, Loss: 0.009259852580726147\n",
            "Epoch 8, Batch 90, Loss: 0.010149450041353703\n",
            "Epoch 8, Batch 100, Loss: 0.012178326025605202\n",
            "Epoch 8, Batch 110, Loss: 0.008491432294249535\n",
            "Epoch 8, Batch 120, Loss: 0.012888552621006966\n",
            "Epoch 8, Batch 130, Loss: 0.011848991736769676\n",
            "Epoch 8, Batch 140, Loss: 0.009340979158878326\n",
            "Epoch 8, Batch 150, Loss: 0.01152466144412756\n",
            "Epoch 8, Batch 160, Loss: 0.014757011085748672\n",
            "Epoch 8, Batch 170, Loss: 0.01148681715130806\n",
            "Epoch 8, Batch 180, Loss: 0.010067700408399105\n",
            "Epoch 8, Batch 190, Loss: 0.012855332344770432\n",
            "Epoch 8, Batch 200, Loss: 0.013082001358270645\n",
            "Epoch 8, Batch 210, Loss: 0.015112929046154022\n",
            "Epoch 8, Batch 220, Loss: 0.01261913776397705\n",
            "Epoch 8, Batch 230, Loss: 0.007869066670536995\n",
            "Epoch 8, Batch 240, Loss: 0.010698163881897926\n",
            "Epoch 8, Batch 250, Loss: 0.007380079012364149\n",
            "Epoch 8, Batch 260, Loss: 0.00946678314357996\n",
            "Epoch 8, Batch 270, Loss: 0.00922923069447279\n",
            "Epoch 8, Batch 280, Loss: 0.010787984356284142\n",
            "Epoch 8, Batch 290, Loss: 0.010585413314402103\n",
            "Epoch 8, Batch 300, Loss: 0.009526487439870834\n",
            "Epoch 8, Batch 310, Loss: 0.012212238274514675\n",
            "Epoch 8, Batch 320, Loss: 0.010715867392718792\n",
            "Epoch 8, Batch 330, Loss: 0.011540580540895462\n",
            "Epoch 8, Batch 340, Loss: 0.01458138506859541\n",
            "Epoch 8, Batch 350, Loss: 0.012741834856569767\n",
            "Epoch 8, Batch 360, Loss: 0.012789573520421982\n",
            "Epoch 8, Batch 370, Loss: 0.008869211189448833\n",
            "Epoch 8, Batch 380, Loss: 0.00926415715366602\n",
            "Epoch 8, Batch 390, Loss: 0.013817591592669487\n",
            "Epoch 8, Batch 400, Loss: 0.006553655955940485\n",
            "Epoch 8, Batch 410, Loss: 0.009949574247002602\n",
            "Epoch 8, Batch 420, Loss: 0.012354378588497639\n",
            "Epoch 8, Batch 430, Loss: 0.010677061975002289\n",
            "Epoch 8, Batch 440, Loss: 0.015889648348093033\n",
            "Epoch 8, Batch 450, Loss: 0.01256825402379036\n",
            "Epoch 8, Batch 460, Loss: 0.009682461619377136\n",
            "Epoch 8, Batch 470, Loss: 0.007385598961263895\n",
            "Epoch 8, Batch 480, Loss: 0.011835926212370396\n",
            "Epoch 8, Batch 490, Loss: 0.011335894465446472\n",
            "Epoch 8, Batch 500, Loss: 0.013567378744482994\n",
            "Epoch 8, Batch 510, Loss: 0.010379349812865257\n",
            "Epoch 8, Batch 520, Loss: 0.009145841002464294\n",
            "Epoch 8, Batch 530, Loss: 0.016015075147151947\n",
            "Epoch 8, Batch 540, Loss: 0.00996486097574234\n",
            "Epoch 8, Batch 550, Loss: 0.010200228542089462\n",
            "Epoch 8, Batch 560, Loss: 0.012397801503539085\n",
            "Epoch 8, Batch 570, Loss: 0.016085520386695862\n",
            "Epoch 8, Batch 580, Loss: 0.012579603120684624\n",
            "Epoch 8, Batch 590, Loss: 0.010472099296748638\n",
            "Epoch 8, Batch 600, Loss: 0.010976201854646206\n",
            "Epoch 8, Batch 610, Loss: 0.01679651252925396\n",
            "Epoch 8, Batch 620, Loss: 0.011926256120204926\n",
            "Epoch 8, Batch 630, Loss: 0.020734917372465134\n",
            "Epoch 8, Batch 640, Loss: 0.013549380004405975\n",
            "Epoch 8, Batch 650, Loss: 0.01459532231092453\n",
            "Epoch 8, Batch 660, Loss: 0.013367164880037308\n",
            "Epoch 8, Batch 670, Loss: 0.012266630306839943\n",
            "Epoch 8, Batch 680, Loss: 0.012971638701856136\n",
            "Epoch 8, Batch 690, Loss: 0.01589932292699814\n",
            "Epoch 8, Batch 700, Loss: 0.0146943936124444\n",
            "Epoch 8, Batch 710, Loss: 0.009646940045058727\n",
            "Epoch 9, Batch 0, Loss: 0.012618149630725384\n",
            "Epoch 9, Batch 10, Loss: 0.008310120552778244\n",
            "Epoch 9, Batch 20, Loss: 0.013361531309783459\n",
            "Epoch 9, Batch 30, Loss: 0.00963754951953888\n",
            "Epoch 9, Batch 40, Loss: 0.009397292509675026\n",
            "Epoch 9, Batch 50, Loss: 0.01264051254838705\n",
            "Epoch 9, Batch 60, Loss: 0.013058330863714218\n",
            "Epoch 9, Batch 70, Loss: 0.016991566866636276\n",
            "Epoch 9, Batch 80, Loss: 0.013345448300242424\n",
            "Epoch 9, Batch 90, Loss: 0.013641562312841415\n",
            "Epoch 9, Batch 100, Loss: 0.007882805541157722\n",
            "Epoch 9, Batch 110, Loss: 0.009898231364786625\n",
            "Epoch 9, Batch 120, Loss: 0.007556863594800234\n",
            "Epoch 9, Batch 130, Loss: 0.01239524595439434\n",
            "Epoch 9, Batch 140, Loss: 0.01239556074142456\n",
            "Epoch 9, Batch 150, Loss: 0.013925301842391491\n",
            "Epoch 9, Batch 160, Loss: 0.01156491506844759\n",
            "Epoch 9, Batch 170, Loss: 0.008459064178168774\n",
            "Epoch 9, Batch 180, Loss: 0.010715611279010773\n",
            "Epoch 9, Batch 190, Loss: 0.012861540541052818\n",
            "Epoch 9, Batch 200, Loss: 0.009898293763399124\n",
            "Epoch 9, Batch 210, Loss: 0.008918577805161476\n",
            "Epoch 9, Batch 220, Loss: 0.011126427911221981\n",
            "Epoch 9, Batch 230, Loss: 0.008821477182209492\n",
            "Epoch 9, Batch 240, Loss: 0.009357819333672523\n",
            "Epoch 9, Batch 250, Loss: 0.014660769142210484\n",
            "Epoch 9, Batch 260, Loss: 0.009800088591873646\n",
            "Epoch 9, Batch 270, Loss: 0.0116078807041049\n",
            "Epoch 9, Batch 280, Loss: 0.009491145610809326\n",
            "Epoch 9, Batch 290, Loss: 0.010964990593492985\n",
            "Epoch 9, Batch 300, Loss: 0.011834651231765747\n",
            "Epoch 9, Batch 310, Loss: 0.012395032681524754\n",
            "Epoch 9, Batch 320, Loss: 0.015917712822556496\n",
            "Epoch 9, Batch 330, Loss: 0.012308657169342041\n",
            "Epoch 9, Batch 340, Loss: 0.007875761017203331\n",
            "Epoch 9, Batch 350, Loss: 0.012025311589241028\n",
            "Epoch 9, Batch 360, Loss: 0.010977331548929214\n",
            "Epoch 9, Batch 370, Loss: 0.010969982482492924\n",
            "Epoch 9, Batch 380, Loss: 0.011734019964933395\n",
            "Epoch 9, Batch 390, Loss: 0.01650858297944069\n",
            "Epoch 9, Batch 400, Loss: 0.014676326885819435\n",
            "Epoch 9, Batch 410, Loss: 0.007251028437167406\n",
            "Epoch 9, Batch 420, Loss: 0.011867462657392025\n",
            "Epoch 9, Batch 430, Loss: 0.013828984461724758\n",
            "Epoch 9, Batch 440, Loss: 0.00940508209168911\n",
            "Epoch 9, Batch 450, Loss: 0.0075126937590539455\n",
            "Epoch 9, Batch 460, Loss: 0.007645659148693085\n",
            "Epoch 9, Batch 470, Loss: 0.009130030870437622\n",
            "Epoch 9, Batch 480, Loss: 0.01523982360959053\n",
            "Epoch 9, Batch 490, Loss: 0.01401897519826889\n",
            "Epoch 9, Batch 500, Loss: 0.01052523497492075\n",
            "Epoch 9, Batch 510, Loss: 0.01073372084647417\n",
            "Epoch 9, Batch 520, Loss: 0.00915025919675827\n",
            "Epoch 9, Batch 530, Loss: 0.01543075405061245\n",
            "Epoch 9, Batch 540, Loss: 0.011835220269858837\n",
            "Epoch 9, Batch 550, Loss: 0.011709429323673248\n",
            "Epoch 9, Batch 560, Loss: 0.011613869108259678\n",
            "Epoch 9, Batch 570, Loss: 0.015535344369709492\n",
            "Epoch 9, Batch 580, Loss: 0.009816918522119522\n",
            "Epoch 9, Batch 590, Loss: 0.011864088475704193\n",
            "Epoch 9, Batch 600, Loss: 0.010922349989414215\n",
            "Epoch 9, Batch 610, Loss: 0.014664718881249428\n",
            "Epoch 9, Batch 620, Loss: 0.010128884576261044\n",
            "Epoch 9, Batch 630, Loss: 0.010007224045693874\n",
            "Epoch 9, Batch 640, Loss: 0.013611351139843464\n",
            "Epoch 9, Batch 650, Loss: 0.01097470335662365\n",
            "Epoch 9, Batch 660, Loss: 0.013561327010393143\n",
            "Epoch 9, Batch 670, Loss: 0.015331469476222992\n",
            "Epoch 9, Batch 680, Loss: 0.012202556245028973\n",
            "Epoch 9, Batch 690, Loss: 0.012218298390507698\n",
            "Epoch 9, Batch 700, Loss: 0.011412388645112514\n",
            "Epoch 9, Batch 710, Loss: 0.016451885923743248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Evaluation on the test dataset\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "total_test_loss = 0.0\n",
        "num_test_samples = 0\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        user, course, grade = batch\n",
        "        output = model(user, course).squeeze()\n",
        "\n",
        "        # Ensure output and grade have the same shape\n",
        "        grade = grade.view(-1, 1)  # Reshape grade to (batch_size, 1) to match output shape\n",
        "\n",
        "        loss = criterion(output, grade)\n",
        "        total_test_loss += loss.item()\n",
        "        num_test_samples += len(grade)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) on the test dataset\n",
        "test_mse = total_test_loss / num_test_samples\n",
        "\n",
        "print(f'Mean Squared Error (MSE) on the test dataset: {test_mse}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9gNce8mLHVW",
        "outputId": "876c022e-8d6f-4ea7-96a9-cc4f66cbd9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) on the test dataset: 0.00023545690095159115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_courses(model, user_and_courses, top_k):\n",
        "    input_user, input_courses = user_and_courses\n",
        "\n",
        "    # Convert input_user to PyTorch tensor\n",
        "    user_ids = torch.LongTensor([input_user])\n",
        "\n",
        "    # Generate all possible course IDs\n",
        "    all_course_ids = torch.arange(len(le_course.classes_))\n",
        "\n",
        "    # Repeat the given user_id for all courses\n",
        "    user_ids = torch.full_like(all_course_ids, fill_value=user_ids[0])\n",
        "\n",
        "    # Make predictions for all courses for the given student\n",
        "    predictions = model(user_ids, all_course_ids).squeeze()\n",
        "\n",
        "    # Get the indices of the top-k predictions\n",
        "    num_recommendations = min(top_k, len(predictions))\n",
        "    top_indices = torch.topk(predictions, num_recommendations).indices\n",
        "\n",
        "    # Reshape top_indices to 1D tensor\n",
        "    top_indices = top_indices.view(-1)\n",
        "\n",
        "    # Map the top indices back to the course IDs\n",
        "    top_course_ids = le_course.inverse_transform(top_indices.numpy())\n",
        "\n",
        "    # Exclude courses already in input_courses from recommendations\n",
        "    recommended_courses = []\n",
        "    unique_course_ids = set()\n",
        "    for course_id in top_course_ids:\n",
        "        if course_id not in input_courses and course_id not in unique_course_ids:\n",
        "            recommended_courses.append(course_id)\n",
        "            unique_course_ids.add(course_id)\n",
        "            if len(recommended_courses) == top_k:\n",
        "                break\n",
        "\n",
        "    return recommended_courses\n"
      ],
      "metadata": {
        "id": "iDwk9mNIUJTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Input student's grade for previous courses\n",
        "input_user = 123  # Replace with the actual student ID\n",
        "\n",
        "# Use the actual course labels seen during training\n",
        "input_courses = {\n",
        "    'CHEMISTRY LABORATORY': 8,\n",
        "    'GENERAL CHEMISTRY': 7,\n",
        "    'ELECTRICAL SCIENCES': 3,\n",
        "    'ADDITIVE MANUFACTURING': 1,\n",
        "    'PRACTICE SCHOOL I':10,\n",
        "    'PHYSICS LABORATORY':5\n",
        "    # Add more courses and grades as needed\n",
        "}\n",
        "\n",
        "# Set top_k to the desired number\n",
        "top_k = 5\n",
        "\n",
        "# Call the function with the updated course grades\n",
        "user_and_courses = (input_user, input_courses)\n",
        "recommended_courses = recommend_courses(model, user_and_courses, top_k)\n",
        "\n",
        "print(f\"Top {top_k} recommended courses for the student based on previous grades: {recommended_courses}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLfO_SpMXikX",
        "outputId": "2feaa57b-dbe1-48d0-d26e-590cdae60736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommended courses for the student based on previous grades: ['APPLIED STOCHASTIC PROCESS', 'ARTIFICIAL INTELLIGENCE FOR RO', 'CINEMATIC ADAPTATION', 'ADV TRANSPORT PHENOMENA', 'ANALOG & DIGIT VLSI DES']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bqcfutgy3qBp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}